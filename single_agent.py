import os
from typing import List, Dict
from dataclasses import dataclass
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from dotenv import load_dotenv
load_dotenv()

@dataclass
class GapAnalysisResult:
    sop_content: str
    gmp_content: str
    gap_type: str
    severity: str
    original_text: str 
    sources: List[str]

class SOPGMPAnalyzer:
    def __init__(self, sop_vector_path: str, gmp_vector_path: str):
        """ì‚¬ìš©ëœ ë²¡í„° ë””ë¹„ íŒŒì¼ : ì „ì²˜ë¦¬ + ì‹œë©˜í‹± ì²­í‚¹"""
        print(f"SOP ë²¡í„° DB ê²½ë¡œ: {sop_vector_path}")
        print(f"GMP ë²¡í„° DB ê²½ë¡œ: {gmp_vector_path}")
        
        print("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask",
            encode_kwargs={'normalize_embeddings': True},
            model_kwargs={'device': 'cpu'},
        )
        
        try:
            self.sop_vectorstore = Chroma(
                persist_directory=sop_vector_path,
                embedding_function=self.embeddings,
                collection_name="sop_documents_semantic"
            )
            print("SOP ë²¡í„° DB ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            print(f"SOP ë²¡í„° DB ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.sop_vectorstore = None
        
        try:
            self.gmp_vectorstore = Chroma(
                persist_directory=gmp_vector_path,
                embedding_function=self.embeddings,
                collection_name="gmp_documents_semantic"
            )
            print("GMP ë²¡í„° DB ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            print(f"GMP ë²¡í„° DB ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.gmp_vectorstore = None
        
        self.llm = ChatOpenAI(temperature=0, model="gpt-4")
        self.check_status()
    
    def check_status(self):
        if self.sop_vectorstore:
            sop_count = self.sop_vectorstore._collection.count()
            print(f"SOP ë¬¸ì„œ ì²­í¬ ìˆ˜: {sop_count}")
        
        if self.gmp_vectorstore:
            gmp_count = self.gmp_vectorstore._collection.count()
            print(f"GMP ë¬¸ì„œ ì²­í¬ ìˆ˜: {gmp_count}")
    
    def compare_section(self, section_topic: str) -> GapAnalysisResult:
        print(f"\në¶„ì„ ì¤‘: {section_topic}")
        print("-" * 50)
        
        # ê²€ìƒ‰
        sop_docs = self.sop_vectorstore.similarity_search(section_topic, k=3) if self.sop_vectorstore else []
        gmp_docs = self.gmp_vectorstore.similarity_search(section_topic, k=3) if self.gmp_vectorstore else []
        
        sop_content = "\n".join([doc.page_content for doc in sop_docs])
        gmp_content = "\n".join([doc.page_content for doc in gmp_docs])
        sop_sources = [doc.metadata.get('source', 'Unknown') for doc in sop_docs]
        gmp_sources = [doc.metadata.get('source', 'Unknown') for doc in gmp_docs]
        
        print(f"SOP ê´€ë ¨ ë¬¸ì„œ: {len(sop_docs)}ê°œ")
        print(f"GMP ê´€ë ¨ ë¬¸ì„œ: {len(gmp_docs)}ê°œ")
        
        if sop_docs:
            print(f"SOP ë¯¸ë¦¬ë³´ê¸°: {sop_docs[0].page_content[:100]}...")
        if gmp_docs:
            print(f"GMP ë¯¸ë¦¬ë³´ê¸°: {gmp_docs[0].page_content[:100]}...")
        
        if not sop_docs and not gmp_docs:
            return GapAnalysisResult(
                sop_content="ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ",
                gmp_content="ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ",
                gap_type="no_data",
                severity="unknown",
                original_text="ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ",
                sources=[]
            )
        
        # LLM ë¶„ì„
        analysis_prompt = f"""
ë‹¤ìŒ SOP ë‚´ìš©ê³¼ GMP/FDA ê·œì •ì„ ë¹„êµí•˜ì—¬ ì°¨ì´ì ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì£¼ì œ: {section_topic}

SOP í˜„í™©:
{sop_content[:1000] if sop_content else "ì—†ìŒ"}

GMP/FDA ê·œì •:
{gmp_content[:1000] if gmp_content else "ì—†ìŒ"}

ë¶„ë¥˜:
- missing: SOPì— GMP ìš”êµ¬ì‚¬í•­ì´ ëˆ„ë½ë¨
- different: SOPì™€ GMP ê·œì •ì´ ë‹¤ë¦„  
- compliant: SOPê°€ GMP ê·œì •ì„ ì˜ ì¤€ìˆ˜

ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€:
Gap Type: [missing/different/compliant]
Severity: [high/medium/low]
Original text: [í•´ë‹¹ë˜ëŠ” GMP ë¬¸ì„œì˜ ì›ë¬¸ ë¬¸ì¥]
"""
        
        try:
            response = self.llm.invoke(analysis_prompt)
            analysis_text = response.content if hasattr(response, 'content') else str(response)
            
            print(f"LLM ë¶„ì„ ê²°ê³¼: {analysis_text[:200]}...")
            
            # íŒŒì‹±
            import re
            gap_match = re.search(r'Gap Type:\s*([^\n\r]+)', analysis_text, re.IGNORECASE)
            severity_match = re.search(r'Severity:\s*([^\n\r]+)', analysis_text, re.IGNORECASE)
            original_match = re.search(r'Original text:\s*(.+?)(?=\n\w+:|$)', analysis_text, re.IGNORECASE | re.DOTALL)
            
            gap_type = gap_match.group(1).strip().lower() if gap_match else "unknown"
            severity = severity_match.group(1).strip().lower() if severity_match else "medium"
            original_text = original_match.group(1).strip() if original_match else "ë¶„ì„ ê²°ê³¼ ì—†ìŒ"
            
            return GapAnalysisResult(
                sop_content=sop_content[:500] + "..." if len(sop_content) > 500 else sop_content,
                gmp_content=gmp_content[:500] + "..." if len(gmp_content) > 500 else gmp_content,
                gap_type=gap_type,
                severity=severity,
                original_text=original_text,
                sources=sop_sources + gmp_sources
            )
            
        except Exception as e:
            print(f"ë¶„ì„ ì‹¤íŒ¨: {e}")
            return GapAnalysisResult(
                sop_content=sop_content[:500] if sop_content else "ì—†ìŒ",
                gmp_content=gmp_content[:500] if gmp_content else "ì—†ìŒ", 
                gap_type="error",
                severity="unknown",
                original_text=f"ë¶„ì„ ì˜¤ë¥˜: {str(e)}",
                sources=sop_sources + gmp_sources
            )
    
    def analyze_multiple_sections(self, sections: List[str]) -> List[GapAnalysisResult]:
        results = []
        for i, section in enumerate(sections, 1):
            print(f"\n[{i}/{len(sections)}] {section} ë¶„ì„")
            result = self.compare_section(section)
            results.append(result)
            
            status = {"compliant": "âœ…", "different": "âš ï¸", "missing": "âŒ", "error": "ğŸš¨"}
            print(f"ê²°ê³¼: {status.get(result.gap_type, 'â“')} {result.gap_type} ({result.severity})")
        
        return results
    
    def generate_report(self, results: List[GapAnalysisResult]) -> str:
        total = len(results)
        compliant = sum(1 for r in results if r.gap_type == "compliant")
        high_risk = sum(1 for r in results if r.severity == "high")
        
        report = f"""
# SOP vs GMP ë¹„êµ ë¶„ì„ ë³´ê³ ì„œ

## ìš”ì•½
- ì „ì²´ ë¶„ì„ í•­ëª©: {total}ê°œ
- ê·œì • ì¤€ìˆ˜: {compliant}ê°œ ({compliant/total*100:.1f}%)
- ê³ ìœ„í—˜ í•­ëª©: {high_risk}ê°œ

## ìƒì„¸ ë¶„ì„ ê²°ê³¼

"""
        
        for i, result in enumerate(results, 1):
            status_icon = {"compliant": "âœ…", "different": "âš ï¸", "missing": "âŒ", "error": "ğŸš¨"}
            icon = status_icon.get(result.gap_type, "â“")
            
            report += f"### {i}. {icon} {result.gap_type.upper()} - {result.severity.upper()}\n"
            report += f"**ê´€ë ¨ ë¬¸ì„œ ì›ë¬¸**: {result.original_text}\n"
            report += f"**ì°¸ê³  ë¬¸ì„œ**: {len(set(result.sources))}ê°œ\n\n"
        
        return report

# ì‚¬ìš©
analyzer = SOPGMPAnalyzer(
    sop_vector_path="/Users/song/Desktop/workspace/final/rag/embedding/chroma_db_sop",
    gmp_vector_path="/Users/song/Desktop/workspace/final/rag/embedding/chroma_db_gmp"
)

results = analyzer.analyze_multiple_sections([
    "ê·œê²©ì¹˜ê°€ nì¸ ê²½ìš° ì‹¤í—˜ì¹˜ëŠ” n+1ìë¦¬ê¹Œì§€ êµ¬í•˜ê³  ì‚¬ì‚¬ì˜¤ì…í•˜ì—¬ ìë¦¬ìˆ˜ë¥¼ ì •í•œë‹¤",
    "í‘œì¤€í’ˆ ì‚¬ìš©ì— ê´€í•œ ê¸°ë¡",
    "ë¡œíŠ¸ ë²ˆí˜¸ ë¶€ì—¬ ì²´ê³„",
    "ì œí’ˆ í‘œì¤€ì„œ ê´€ë¦¬ìëŠ” ì œí’ˆ í‘œì¤€ì„œ ì›ë³¸ì˜ ê°œì • ë‚´ì—­ì— ì‹ ê·œ ì›ë£Œê°€ ì ìš©ë˜ê¸° ì‹œì‘í•œ ë³¸ ìƒì‚° ì œì¡°ë²ˆí˜¸ ë° ìƒì‚°ì¼ì„ ê¸°ë¡í•œë‹¤"
])

report = analyzer.generate_report(results)
print(report)