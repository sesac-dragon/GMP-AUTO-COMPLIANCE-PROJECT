# ì‹¤í–‰ ë°©ë²•
#conda activate [ê°€ìƒí™˜ê²½]
#cd C:\Users\MASTER\Desktop\text_extraction_chunking
#python -m pip install -r requirements.txt

#python extract_and_chunk_pdfs.py --zip "C:\Users\MASTER\Desktop\text_extraction_chunking\data.zip" ^
#  --out "C:\Users\MASTER\Desktop\text_extraction_chunking\chunks.jsonl" ^
#  --backend pypdf --chunk-by regsection --chunk-size 1400 --overlap 120 ^
#  --jurisdiction-from-path

# chunking ì™„ë£Œëœ ê²°ê³¼ë¬¼ì€ jsonìœ¼ë¡œ ë°”íƒ•í™”ë©´ì— ì €ì¥ë¨


# -*- coding: utf-8 -*-
"""
GMP PDF â†’ Text â†’ Clean â†’ Chunk â†’ JSONL íŒŒì´í”„ë¼ì¸ (v2 + regsection)

í•µì‹¬
- ê¸°ë³¸ ì¶”ì¶œ ë°±ì—”ë“œ: pypdf ìš°ì„ , í•„ìš” ì‹œ pdfminer.six í´ë°±
- --backend {auto|pypdf|pdfminer}
- --chunk-by {auto,paragraph,sentence,char,regsection}
- regsection: Annex/Section/Â§/1.2.3/ì œnì¡° ë“± "ì¡°í•­/ì„¹ì…˜" ê²½ê³„ë¥¼ ì¸ì‹í•´ ë¸”ë¡ ë‹¨ìœ„ ì²­í‚¹
  (ë„ˆë¬´ í¬ë©´ í•˜ìœ„ í•­ëª© ((1)/(a)/1.) ê¸°ì¤€ 2ì°¨ ë¶„í• , overlapì€ ê°™ì€ ì¡°í•­ ë‚´ì—ì„œë§Œ)

ë©”íƒ€ë°ì´í„° ê°•í™”
- jurisdiction(EU/FDA/WHO/PIC/S/MFDS ë“± ê²½ë¡œ ê¸°ë°˜ ì¶”ì • ì˜µì…˜)
- doc_date, doc_version(ë³¸ë¬¸/íŒŒì¼ëª…ì—ì„œ íœ´ë¦¬ìŠ¤í‹± ì¶”ì •)
- source_url(ì„ íƒ: JSONL/CSV ì†ŒìŠ¤ë§µë¡œ ì£¼ì…)
- section_id, section_title, normative_strength(MUST/SHOULD/MAY)

ì‹¤í–‰ ì˜ˆì‹œ (ê¶Œì¥)
  python extract_and_chunk_pdfs.py --zip data.zip --out chunks.jsonl \
    --backend pypdf --chunk-by regsection --chunk-size 1400 --overlap 120 \
    --jurisdiction-from-path
"""

from __future__ import annotations
import argparse
import csv
import json
import logging
import hashlib
import os
import re
import sys
import time
import zipfile
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, List, Optional, Tuple

# ---------- Logging ----------
logging.getLogger("pypdf").setLevel(logging.ERROR)

def log(msg: str) -> None:
    ts = time.strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{ts}] {msg}", flush=True)

def slugify(text: str, maxlen: int = 80) -> str:
    s = re.sub(r"[^\wê°€-í£\-_. ]+", "", text).strip()
    s = re.sub(r"\s+", "_", s)
    return s[:maxlen] if len(s) > maxlen else s

def stable_hash(text: str, n: int = 16) -> str:
    return hashlib.sha1(text.encode("utf-8", errors="ignore")).hexdigest()[:n]

# ---------- Backend ----------
class PDFTextExtractor:
    """
    pypdf â†’ pdfminer ìˆœìœ¼ë¡œ ì‹œë„ (ê¸°ë³¸).
    --backendë¡œ ê°•ì œ ê°€ëŠ¥.
    """
    def __init__(self, backend: str = "auto"):
        self.backend_pref = backend  # auto|pypdf|pdfminer
        self.has_pypdf = self._check_pypdf()
        self.has_pdfminer = self._check_pdfminer()
        if backend not in {"auto", "pypdf", "pdfminer"}:
            raise ValueError("--backend must be one of auto|pypdf|pdfminer")
        if backend == "pypdf" and not self.has_pypdf:
            raise RuntimeError("pypdf is not installed")
        if backend == "pdfminer" and not self.has_pdfminer:
            raise RuntimeError("pdfminer.six is not installed")
        if backend == "auto" and not (self.has_pypdf or self.has_pdfminer):
            raise RuntimeError("Install pypdf or pdfminer.six")

    def _check_pypdf(self) -> bool:
        try:
            import pypdf  # noqa
            return True
        except Exception:
            return False

    def _check_pdfminer(self) -> bool:
        try:
            import pdfminer  # noqa
            return True
        except Exception:
            return False

    def extract_page_texts(self, pdf_path: Path) -> List[str]:
        """
        pypdf ë¨¼ì € â†’ ë¹„ì–´ìˆëŠ” í˜ì´ì§€ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ pdfminer í´ë°±.
        --backend=pdfminerë©´ pdfminerë§Œ ì‚¬ìš©, --backend=pypdfë©´ pypdfë§Œ ì‚¬ìš©.
        """
        order: List[str]
        if self.backend_pref == "pypdf":
            order = ["pypdf"]
        elif self.backend_pref == "pdfminer":
            order = ["pdfminer"]
        else:
            order = ["pypdf", "pdfminer"]

        last_exc: Optional[Exception] = None
        for be in order:
            try:
                if be == "pypdf" and self.has_pypdf:
                    texts = self._extract_with_pypdf(pdf_path)
                    # ë¹ˆ í˜ì´ì§€ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ í’ˆì§ˆ ì €í•˜ë¡œ ê°„ì£¼í•˜ê³  í´ë°±
                    if texts and (sum(1 for t in texts if t and t.strip()) / max(1, len(texts)) >= 0.3):
                        return texts
                if be == "pdfminer" and self.has_pdfminer:
                    texts = self._extract_with_pdfminer(pdf_path)
                    return texts
            except KeyboardInterrupt:
                raise
            except Exception as e:
                last_exc = e
                continue
        if last_exc:
            raise last_exc
        return []

    def _extract_with_pypdf(self, pdf_path: Path) -> List[str]:
        import pypdf
        texts: List[str] = []
        with open(pdf_path, "rb") as f:
            reader = pypdf.PdfReader(f, strict=False)
            for page in reader.pages:
                try:
                    t = page.extract_text() or ""
                except Exception:
                    t = ""
                texts.append(t)
        return texts

    def _extract_with_pdfminer(self, pdf_path: Path) -> List[str]:
        from pdfminer.high_level import extract_text
        from pdfminer.pdfpage import PDFPage
        texts: List[str] = []
        # í˜ì´ì§€ ëª©ë¡ì„ ë¨¼ì € ê°€ì ¸ì™€ ì¸ë±ìŠ¤ë¡œ ê°œë³„ ì¶”ì¶œ (ë¬´í•œ ëŒ€ê¸° ë°©ì§€ ì¼í™˜)
        with open(pdf_path, "rb") as fp:
            page_iter = list(PDFPage.get_pages(fp))
        for i in range(len(page_iter)):
            try:
                t = extract_text(str(pdf_path), page_numbers=[i]) or ""
            except Exception:
                t = ""
            texts.append(t)
        return texts

# ---------- Normalize / Header-Footer ----------
def normalize_text(s: str) -> str:
    s = s.replace("\ufeff", "").replace("\t", " ")
    # í•˜ì´í”ˆ ì¤„ë°”ê¿ˆ ë‹¨ì–´ ì´ì–´ë¶™ì´ê¸° (ì˜ë¬¸ ì¤‘ì‹¬)
    s = re.sub(r"(\w)-\n(\w)", r"\1\2", s)
    # ë‹¤ì¤‘ ê³µë°± ì¶•ì•½
    s = re.sub(r"[ \u00A0]{2,}", " ", s)
    # ì¤„ë°”ê¿ˆ ì •ê·œí™”
    s = s.replace("\r\n", "\n").replace("\r", "\n")
    # ê³¼ë„í•œ ë¹ˆ ì¤„ ì¶•ì•½
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

def detect_repeating_lines(
    pages: List[str], top_k: int = 3, bottom_k: int = 3, freq_threshold: float = 0.4
):
    from collections import Counter
    head_counter, foot_counter = Counter(), Counter()
    for page in pages:
        lines = [ln.strip() for ln in page.splitlines() if ln.strip()]
        if not lines:
            continue
        heads = lines[:min(top_k, len(lines))]
        foots = lines[-min(bottom_k, len(lines)):]
        for h in heads:
            head_counter[h] += 1
        for f in foots:
            foot_counter[f] += 1
    n_pages = max(1, len(pages))
    head_repeats = {l for l, c in head_counter.items() if c / n_pages >= freq_threshold}
    foot_repeats = {l for l, c in foot_counter.items() if c / n_pages >= freq_threshold}
    return head_repeats, foot_repeats

def remove_headers_footers(pages: List[str]) -> List[str]:
    head_repeats, foot_repeats = detect_repeating_lines(pages)
    cleaned = []
    for page in pages:
        lines = [ln for ln in page.splitlines()]
        while lines and lines[0].strip() in head_repeats:
            lines.pop(0)
        while lines and lines[-1].strip() in foot_repeats:
            lines.pop()
        cleaned.append("\n".join(lines))
    return cleaned

# ---------- Basic splitters ----------
_SENTENCE_SPLIT_REGEX = re.compile(
    r"(?<=[\.!\?ã€‚ï¼ï¼Ÿ])\s+|(?<=\.)\s+|(?<=\))\s+\n|(?<=\])\s+\n"
)

def split_into_paragraphs(text: str) -> List[str]:
    paras = [p.strip() for p in text.split("\n\n")]
    return [p for p in paras if p]

def split_into_sentences(text: str) -> List[str]:
    parts = _SENTENCE_SPLIT_REGEX.split(text)
    merged, buf = [], []
    for part in parts:
        t = part.strip()
        if not t:
            continue
        buf.append(t)
        if sum(len(x) for x in buf) > 80:
            merged.append(" ".join(buf))
            buf = []
    if buf:
        merged.append(" ".join(buf))
    return merged

# ---------- RegSection (ê·œì •/ì¡°í•­ ì¸ì§€í˜•) ----------
# ê°•í•œ í—¤ë”©(ì„¹ì…˜ ì‹œì‘) íŒ¨í„´
RE_STRONG_HEAD = re.compile(
    r"^(?:(Annex|Appendix|Part|Chapter|Section|Clause)\s+([\w\.\-]+)\b\s*(.*)$"
    r"|Â§\s*([\d\.]+)\b\s*(.*)$"
    r"|([0-9]+(?:\.[0-9]+)+)\b\s*(.+)$"
    r"|ì œ\s*(\d+)\s*(ì¥|ì ˆ|ì¡°)\s*(.*)$)",
    re.IGNORECASE
)
# ë„ˆë¬´ í° ì„¹ì…˜ì˜ 2ì°¨ ë¶„í•  ê¸°ì¤€ (í•˜ìœ„ í•­)
RE_SUBCLAUSE = re.compile(r"^(\(\d+\)|\([a-zA-Z]\)|[0-9]+\.)\s+.+")  # (1) (a) 1. ë“±

@dataclass
class RegUnit:
    section_id: Optional[str]
    section_title: Optional[str]
    text: str
    start_pos: int  # full_text ìƒ ë¬¸ì ì˜¤í”„ì…‹ ê¸°ì¤€
    end_pos: int

def split_regsections(full_text: str) -> List[RegUnit]:
    """
    ê°•í•œ í—¤ë”©ì„ ê¸°ì¤€ìœ¼ë¡œ ì„¹ì…˜ ë‹¨ìœ„ ë¶„í• .
    ë„ˆë¬´ í° ì„¹ì…˜ì€ í•˜ìœ„ í•­ìœ¼ë¡œ ì¬ë¶„í• .
    """
    lines = full_text.splitlines()
    units: List[RegUnit] = []
    cur_lines: List[str] = []
    cur_id, cur_title = None, None
    pos_offset = 0
    cur_start = 0

    def flush_unit(end_pos: int):
        nonlocal cur_lines, cur_id, cur_title, cur_start
        if not cur_lines:
            return
        text = "\n".join(cur_lines).strip()
        if text:
            units.append(RegUnit(cur_id, cur_title, text, cur_start, end_pos))
        cur_lines = []

    for ln in lines:
        m = RE_STRONG_HEAD.match(ln.strip())
        line_len = len(ln) + 1  # + newline
        if m:
            # ì´ì „ ì„¹ì…˜ ì¢…ë£Œ
            flush_unit(pos_offset - 1)
            # ID/ì œëª© íŒŒì‹±
            if m.group(1):  # Annex/Appendix/Part/Chapter/Section/Clause + ì½”ë“œ + ì œëª©
                cur_id = f"{m.group(1).title()} {m.group(2)}".strip()
                cur_title = (m.group(3) or "").strip()
            elif m.group(4):  # Â§ 1.2.3
                cur_id = f"Â§ {m.group(4)}"
                cur_title = (m.group(5) or "").strip()
            elif m.group(6):  # 1.2.3 ì œëª©
                cur_id = m.group(6)
                cur_title = (m.group(7) or "").strip()
            elif m.group(8):  # ì œ n (ì¥|ì ˆ|ì¡°) ì œëª©
                cur_id = f"ì œ{m.group(8)}{m.group(9)}"
                cur_title = (m.group(10) or "").strip()
            else:
                cur_id, cur_title = None, None
            cur_start = pos_offset
            cur_lines = [ln]
        else:
            if not cur_lines:
                # ì„œë¬¸/ë¨¸ë¦¬ë§ ë¸”ë¡
                cur_id, cur_title = "PREFACE", None
                cur_start = pos_offset
            cur_lines.append(ln)
        pos_offset += line_len

    flush_unit(pos_offset)

    # í° ì„¹ì…˜ ì¬ë¶„í• 
    refined: List[RegUnit] = []
    for u in units:
        if len(u.text) <= 1600:
            refined.append(u)
            continue
        sublines = u.text.splitlines()
        cur_sub: List[str] = []
        sub_start_off = u.start_pos
        local_offset = 0

        def flush_sub(end_off: int):
            nonlocal cur_sub, sub_start_off
            if cur_sub:
                t = "\n".join(cur_sub).strip()
                if t:
                    refined.append(RegUnit(u.section_id, u.section_title, t, sub_start_off, end_off))
                cur_sub = []

        for ln in sublines:
            m = RE_SUBCLAUSE.match(ln.strip())
            line_len = len(ln) + 1
            if m and cur_sub:
                flush_sub(u.start_pos + local_offset - 1)
                sub_start_off = u.start_pos + local_offset
                cur_sub = [ln]
            else:
                cur_sub.append(ln)
            local_offset += line_len
        flush_sub(u.start_pos + local_offset)

    return refined

# ---------- Chunking ----------
def chunk_text_units(units: List[RegUnit], chunk_size: int, overlap: int) -> List[RegUnit]:
    """
    ê° ì„¹ì…˜(RegUnit) ë‚´ë¶€ì—ì„œë§Œ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì²­í‚¹.
    ì„¹ì…˜ ê²½ê³„ë¥¼ ë„˜ì§€ ì•ŠìŒ.
    """
    chunks: List[RegUnit] = []
    for u in units:
        t = u.text
        if len(t) <= chunk_size * 1.1:
            chunks.append(u)
            continue
        step = chunk_size - overlap if overlap < chunk_size else chunk_size
        i = 0
        while i < len(t):
            sub = t[i : i + chunk_size]
            start_pos = u.start_pos + i
            end_pos = min(u.start_pos + i + len(sub), u.end_pos)
            chunks.append(RegUnit(u.section_id, u.section_title, sub, start_pos, end_pos))
            i += step
    return chunks

def chunk_text(text: str, chunk_size: int = 1200, overlap: int = 150, by: str = "auto") -> List[str]:
    """
    ê¸°ë³¸ ëª¨ë“œ(ë¬¸ë‹¨/ë¬¸ì¥/ë¬¸ì) ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì²­í‚¹.
    """
    text = normalize_text(text)
    if not text:
        return []
    if by == "paragraph" or (by == "auto" and len(text) > chunk_size * 1.5):
        units = split_into_paragraphs(text)
    elif by == "sentence":
        units = split_into_sentences(text)
    elif by == "char":
        units = [text]
    else:
        # auto: ë¬¸ë‹¨ ìš°ì„ , í° ë¬¸ë‹¨ì€ ë¬¸ì¥ ì„¸ë¶„í™”
        units = split_into_paragraphs(text)
        refined = []
        for u in units:
            if len(u) > chunk_size * 1.2:
                refined.extend(split_into_sentences(u))
            else:
                refined.append(u)
        units = refined

    chunks: List[str] = []
    buf = ""
    for u in units:
        if not buf:
            buf = u
        elif len(buf) + 1 + len(u) <= chunk_size:
            buf = f"{buf}\n{u}"
        else:
            chunks.append(buf.strip())
            tail = buf[-overlap:] if overlap > 0 else ""
            buf = (tail + "\n" + u).strip()
    if buf:
        chunks.append(buf.strip())

    # ë¹„ìƒ ë¬¸ì ë¶„í• 
    final_chunks: List[str] = []
    for ck in chunks:
        if len(ck) <= chunk_size * 1.5:
            final_chunks.append(ck)
        else:
            s = ck
            step = chunk_size - overlap if overlap < chunk_size else chunk_size
            i = 0
            while i < len(s):
                final_chunks.append(s[i : i + chunk_size])
                i += step
    return final_chunks

# ---------- ZIP & Files ----------
def extract_zip_to_dir(zip_path: Path, out_dir: Path) -> None:
    out_dir.mkdir(parents=True, exist_ok=True)
    with zipfile.ZipFile(zip_path, "r") as zf:
        zf.extractall(out_dir)

def find_pdfs(root: Path) -> List[Path]:
    return sorted([p for p in root.rglob("*.pdf") if p.is_file()])

# ---------- Metadata helpers ----------
def extract_doc_meta(pages: List[str], filename: str) -> Tuple[Optional[str], Optional[str]]:
    """
    ì•ìª½ í˜ì´ì§€/íŒŒì¼ëª…ì—ì„œ ë°œí–‰ì¼/ë²„ì „ ì¶”ì •.
    """
    head_text = "\n".join(pages[:3])
    # ë‚ ì§œ: 2020-12-31 / 2020.12.31 / 2020/12/31 / 20 Aug 2023
    m = re.search(
        r"(20\d{2}[./\- ]\d{1,2}[./\- ]\d{1,2}|[0-3]?\d\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+20\d{2})",
        head_text,
        re.IGNORECASE,
    )
    date_val = m.group(0) if m else None
    # ë²„ì „: Rev 3.1 / Version 2 / ver.1.0
    mv = re.search(r"\b(Rev(?:ision)?|Version|Ver\.?)\s*[:\-]?\s*([A-Za-z]?\d+(?:\.\d+)*)", head_text, re.IGNORECASE)
    ver_val = (mv.group(1) + " " + mv.group(2)) if mv else None
    # íŒŒì¼ëª… ë³´ì¡°
    if not date_val:
        mf = re.search(r"(20\d{2}[._-]\d{1,2}[._-]\d{1,2}|20\d{2})", filename)
        date_val = mf.group(0) if mf else None
    if not ver_val:
        mvf = re.search(r"(Rev(?:ision)?|Version|Ver)[._ -]*([A-Za-z]?\d+(?:\.\d+)*)", filename, re.IGNORECASE)
        ver_val = (mvf.group(1) + " " + mvf.group(2)) if mvf else None
    return date_val, ver_val

def infer_jurisdiction(path_str: str) -> Optional[str]:
    s = path_str.lower()
    if any(k in s for k in ["eu", "ema"]):
        return "EU"
    if any(k in s for k in ["usfda", "fda", "cfr", "21 cfr", "21cfr"]):
        return "US-FDA"
    if "who" in s:
        return "WHO"
    if "pic" in s:
        return "PIC/S"
    if any(k in s for k in ["mfds", "kfds", "korea"]):
        return "KR-MFDS"
    return None

def label_normative_strength(text: str) -> Optional[str]:
    """
    MUST/SHOULD/MAY ê°„ë‹¨ ë¼ë²¨ë§ (ì˜ë¬¸/êµ­ë¬¸ í‚¤ì›Œë“œ)
    """
    tl = " " + text.lower() + " "
    strong = sum(x in tl for x in [" shall ", " must ", " required ", " require "])
    should = sum(x in tl for x in [" should ", " recommended ", " recommend ", " ought "])
    may = sum(x in tl for x in [" may ", " can ", " optional "])
    # í•œê¸€
    strong += sum(x in text for x in ["í•˜ì—¬ì•¼ í•œë‹¤", "í•´ì•¼ í•œë‹¤", "í•´ì•¼í•œë‹¤", "í•„ìˆ˜", "ì˜ë¬´"])
    should += sum(x in text for x in ["ê¶Œì¥", "ë°”ëŒì§", "ê¶Œê³ "])
    may += sum(x in text for x in ["í•  ìˆ˜ ìˆë‹¤", "ê°€ëŠ¥"])
    if strong >= should and strong >= may and strong > 0:
        return "MUST"
    if should >= strong and should >= may and should > 0:
        return "SHOULD"
    if may > 0:
        return "MAY"
    return None

# ---------- Output record ----------
@dataclass
class ChunkRecord:
    id: str
    doc_id: str
    source_path: str
    title: str
    jurisdiction: Optional[str]
    doc_date: Optional[str]
    doc_version: Optional[str]
    source_url: Optional[str]
    section_id: Optional[str]
    section_title: Optional[str]
    normative_strength: Optional[str]
    page_start: int
    page_end: int
    chunk_index: int
    text: str

def write_jsonl(path: Path, records: Iterable[ChunkRecord]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        for r in records:
            f.write(
                json.dumps(
                    {
                        "id": r.id,
                        "doc_id": r.doc_id,
                        "source_path": r.source_path,
                        "title": r.title,
                        "jurisdiction": r.jurisdiction,
                        "doc_date": r.doc_date,
                        "doc_version": r.doc_version,
                        "source_url": r.source_url,
                        "section_id": r.section_id,
                        "section_title": r.section_title,
                        "normative_strength": r.normative_strength,
                        "page_start": r.page_start,
                        "page_end": r.page_end,
                        "chunk_index": r.chunk_index,
                        "text": r.text,
                    },
                    ensure_ascii=False,
                )
                + "\n"
            )

# ---------- Source map (optional) ----------
def load_source_map(path: Optional[Path]) -> dict:
    """
    JSONL/CSV ì§€ì›: key(path|filename|stem) -> {source_url, doc_date, doc_version}
    """
    if not path:
        return {}
    mp: dict = {}
    if path.suffix.lower() == ".jsonl":
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    o = json.loads(line)
                except Exception:
                    continue
                key = o.get("path") or o.get("filename") or o.get("stem")
                if key:
                    mp[str(key)] = {
                        "source_url": o.get("source_url"),
                        "doc_date": o.get("doc_date"),
                        "doc_version": o.get("doc_version"),
                    }
    elif path.suffix.lower() == ".csv":
        with open(path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for o in reader:
                key = o.get("path") or o.get("filename") or o.get("stem")
                if key:
                    mp[str(key)] = {
                        "source_url": o.get("source_url"),
                        "doc_date": o.get("doc_date"),
                        "doc_version": o.get("doc_version"),
                    }
    return mp

# ---------- Build per PDF ----------
def build_chunks_for_pdf(
    pdf_path: Path,
    extractor: PDFTextExtractor,
    chunk_size: int,
    overlap: int,
    chunk_by: str,
    jurisdiction_from_path: bool,
    source_map: dict,
) -> List[ChunkRecord]:
    rel_title = pdf_path.stem
    doc_id = f"{slugify(rel_title)}-{stable_hash(str(pdf_path))}"

    try:
        pages = extractor.extract_page_texts(pdf_path)
    except KeyboardInterrupt:
        raise
    except Exception as e:
        log(f"âš ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {pdf_path.name} ({e})")
        return []

    if not any(p.strip() for p in pages):
        log(f"ğŸ” OCR í•„ìš” ê°€ëŠ¥ì„±(ë¹ˆ í…ìŠ¤íŠ¸): {pdf_path.name}")
        return []

    # ì •ì œ + í—¤ë”/í‘¸í„° ì œê±°
    pages_norm = [normalize_text(p) for p in pages]
    pages_clean = remove_headers_footers(pages_norm)

    # í˜ì´ì§€ ì‚¬ì´ì— \n\n ë¥¼ ë„£ì–´ full_text êµ¬ì„±
    if pages_clean:
        full_text = ("\n\n").join(pages_clean).strip()
    else:
        full_text = ""

    # ë©”íƒ€ë°ì´í„° ì¶”ì •
    doc_date, doc_version = extract_doc_meta(pages_clean, pdf_path.name)
    jurisdiction = infer_jurisdiction(str(pdf_path)) if jurisdiction_from_path else None

    # ì™¸ë¶€ ì†ŒìŠ¤ë§µ ë³‘í•©
    url_info = (
        source_map.get(str(pdf_path))
        or source_map.get(pdf_path.name)
        or source_map.get(pdf_path.stem)
        or {}
    )
    source_url = url_info.get("source_url")
    doc_date = url_info.get("doc_date") or doc_date
    doc_version = url_info.get("doc_version") or doc_version

    # í˜ì´ì§€ ë§¤í•‘: full_textì— ì‚½ì…í•œ í˜ì´ì§€ êµ¬ë¶„(\n\n) 2ê¸€ìë„ ëˆ„ì ì— ë°˜ì˜
    page_bounds: List[Tuple[int, int]] = []
    acc = 0
    for i, p in enumerate(pages_clean, 1):
        acc += len(p)
        if i < len(pages_clean):
            acc += 2  # "\n\n"
        page_bounds.append((i, acc))

    def guess_page_range(start_pos: int, end_pos: int) -> Tuple[int, int]:
        def pos_to_page(pos: int) -> int:
            for pg, cumlen in page_bounds:
                if pos <= cumlen:
                    return pg
            return len(page_bounds)
        return pos_to_page(start_pos), pos_to_page(end_pos)

    records: List[ChunkRecord] = []

    if chunk_by == "regsection":
        # ì„¹ì…˜/ì¡°í•­ ì¸ì§€í˜• ë¶„í•  â†’ ì„¹ì…˜ ë‚´ ìœˆë„ìš° ì²­í‚¹
        units = split_regsections(full_text)
        units = chunk_text_units(units, chunk_size, overlap)
        for idx, u in enumerate(units):
            p_s, p_e = guess_page_range(u.start_pos, u.end_pos)
            chunk_id = f"{doc_id}-{idx:04d}"
            records.append(
                ChunkRecord(
                    id=chunk_id,
                    doc_id=doc_id,
                    source_path=str(pdf_path),
                    title=rel_title,
                    jurisdiction=jurisdiction,
                    doc_date=doc_date,
                    doc_version=doc_version,
                    source_url=source_url,
                    section_id=u.section_id,
                    section_title=u.section_title,
                    normative_strength=label_normative_strength(u.text),
                    page_start=p_s,
                    page_end=p_e,
                    chunk_index=idx,
                    text=u.text,
                )
            )
        return records

    # ê¸°ë³¸ ëª¨ë“œ(ë¬¸ë‹¨/ë¬¸ì¥/ë¬¸ì/auto)
    chunks = chunk_text(full_text, chunk_size=chunk_size, overlap=overlap, by=chunk_by)
    # ì˜¤í”„ì…‹ ê·¼ì‚¬ ì¶”ì •ìœ¼ë¡œ í˜ì´ì§€ ë§¤í•‘
    offset = 0
    for idx, ck in enumerate(chunks):
        start_pos = offset
        end_pos = offset + len(ck)
        p_s, p_e = guess_page_range(start_pos, end_pos)
        chunk_id = f"{doc_id}-{idx:04d}"
        records.append(
            ChunkRecord(
                id=chunk_id,
                doc_id=doc_id,
                source_path=str(pdf_path),
                title=rel_title,
                jurisdiction=jurisdiction,
                doc_date=doc_date,
                doc_version=doc_version,
                source_url=source_url,
                section_id=None,
                section_title=None,
                normative_strength=label_normative_strength(ck),
                page_start=p_s,
                page_end=p_e,
                chunk_index=idx,
                text=ck,
            )
        )
        # overlapì„ ê³ ë ¤í•œ ë‹¤ìŒ ì‹œì‘ ìœ„ì¹˜
        offset = end_pos - min(overlap, len(ck))

    return records

# ---------- Main ----------
def main():
    ap = argparse.ArgumentParser(description="PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í‚¹ â†’ JSONL ìƒì„± (v2 + regsection)")
    src = ap.add_mutually_exclusive_group(required=True)
    src.add_argument("--zip", type=str, help="PDF ZIP ê²½ë¡œ")
    src.add_argument("--pdf-dir", type=str, help="PDF í´ë” ê²½ë¡œ")

    ap.add_argument("--workdir", type=str, default="./work", help="ZIP í•´ì œ í´ë”")
    ap.add_argument("--out", type=str, default="./chunks.jsonl", help="ì¶œë ¥ JSONL ê²½ë¡œ")
    ap.add_argument("--chunk-size", type=int, default=1400, help="ì²­í¬ ëª©í‘œ ê¸€ì ìˆ˜")
    ap.add_argument("--overlap", type=int, default=120, help="ì²­í¬ ê°„ ê²¹ì¹¨(ë¬¸ì)")
    ap.add_argument(
        "--chunk-by",
        type=str,
        default="regsection",
        choices=["auto", "paragraph", "sentence", "char", "regsection"],
        help="ì²­í‚¹ ê¸°ì¤€",
    )
    ap.add_argument(
        "--backend",
        type=str,
        default="auto",
        choices=["auto", "pypdf", "pdfminer"],
        help="í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°±ì—”ë“œ",
    )
    ap.add_argument(
        "--jurisdiction-from-path",
        action="store_true",
        help="ê²½ë¡œëª…ìœ¼ë¡œ ê´€í• ê¸°ê´€ ì¶”ì •(EU/FDA/WHO/PIC/S/MFDS)",
    )
    ap.add_argument(
        "--source-map",
        type=str,
        default=None,
        help="JSONL/CSV: path|filename|stem -> {source_url, doc_date, doc_version}",
    )
    args = ap.parse_args()

    # ë°±ì—”ë“œ ì¤€ë¹„
    try:
        extractor = PDFTextExtractor(backend=args.backend)
    except Exception as e:
        log(f"âŒ ë°±ì—”ë“œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        sys.exit(1)

    # ì…ë ¥ ì¤€ë¹„
    if args.zip:
        zip_path = Path(args.zip)
        if not zip_path.exists():
            log(f"âŒ ZIP ì—†ìŒ: {zip_path}")
            sys.exit(1)
        workdir = Path(args.workdir)
        log(f"ğŸ“¦ ZIP í•´ì œ: {zip_path} â†’ {workdir}")
        extract_zip_to_dir(zip_path, workdir)
        pdf_root = workdir
    else:
        pdf_root = Path(args.pdf_dir)
        if not pdf_root.exists():
            log(f"âŒ í´ë” ì—†ìŒ: {pdf_root}")
            sys.exit(1)

    source_map = load_source_map(Path(args.source_map)) if args.source_map else {}

    pdfs = find_pdfs(pdf_root)
    if not pdfs:
        log("âŒ PDFê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    log(f"ğŸ“„ PDF ê°œìˆ˜: {len(pdfs)}")
    all_records: List[ChunkRecord] = []
    for i, pdf in enumerate(pdfs, 1):
        log(f"[{i}/{len(pdfs)}] ì²˜ë¦¬ì¤‘: {pdf}")
        recs = build_chunks_for_pdf(
            pdf,
            extractor,
            args.chunk_size,
            args.overlap,
            args.chunk_by,
            args.jurisdiction_from_path,
            source_map,
        )
        all_records.extend(recs)
        log(f"   â†’ ì²­í¬ {len(recs)}ê°œ")

    out_path = Path(args.out)
    write_jsonl(out_path, all_records)
    log(f"âœ… ì™„ë£Œ: {out_path} (ì´ ì²­í¬ {len(all_records)}ê°œ)")

if __name__ == "__main__":
    main()
